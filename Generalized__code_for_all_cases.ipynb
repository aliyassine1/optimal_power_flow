{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Generalized._codeipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flj3XAengn13"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH8jG1seg7zT"
      },
      "source": [
        "Case Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q2VTfQRg6U-",
        "outputId": "1f44578d-b70c-4fac-a246-07b95e021728"
      },
      "source": [
        " pip install PYPOWER #installing PyPower"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PYPOWER\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/46/8db1ade05ab6eaa218a319a9fe7b9077706f92b67d8d1d2018dbc805ae55/PYPOWER-5.1.15-py2.py3-none-any.whl (346kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 20.3MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 16.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 40kB 14.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 51kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 61kB 11.2MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 71kB 10.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 92kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 307kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 317kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 327kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 337kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 348kB 9.1MB/s \n",
            "\u001b[?25hInstalling collected packages: PYPOWER\n",
            "Successfully installed PYPOWER-5.1.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2liXnMErjkp"
      },
      "source": [
        "from pypower.loadcase import loadcase\n",
        "from pypower.makeYbus import makeYbus\n",
        "import pypower\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEBX_pNPvoZE"
      },
      "source": [
        "Accessing the Pmin and Pmax from the case and getting the slack bus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8KUwA0yvpbl"
      },
      "source": [
        "def Initialize(ppc):\n",
        "  Gen = ppc[\"gen\"]\n",
        "  Bus =[]\n",
        "  PMax= []\n",
        "  PMin=[]\n",
        "  i= 0 #iterator over every generator to get the Bus, Pmax , Pmin of the generator\n",
        "\n",
        "  while (i!= len(Gen)):\n",
        "    Bus.append(ppc[\"gen\"][i,0])\n",
        "    PMax.append(ppc[\"gen\"][i,8])\n",
        "    PMin.append(ppc[\"gen\"][i,9])\n",
        "    i = i+1\n",
        "\n",
        "  Power= pd.DataFrame(columns=['Bus','Pmin','Pmax'])\n",
        "  Power['Bus']= Bus\n",
        "  Power['Pmin'] = PMin\n",
        "  Power['Pmax'] = PMax \n",
        "  Power.set_index('Bus')\n",
        "  #print(Power)\n",
        "  #finding the slack bus\n",
        "\n",
        "  slack = 0\n",
        "  j=0 #iterator over every bus row to check for the bus type\n",
        "  while(slack == 0):\n",
        "    if(ppc[\"bus\"][j,1]== 3):\n",
        "      slack = ppc[\"bus\"][j,0]\n",
        "    j +=1\n",
        "  #print(slack)\n",
        "  slack_adapted = int(slack) - 1  \n",
        "  return Power,slack, slack_adapted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yul1bJrjxJlI"
      },
      "source": [
        "Getting the admittance matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EbYy6axMOP"
      },
      "source": [
        "from pypower.ext2int import ext2int\n",
        "from pypower.makeBdc import makeBdc\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def GetAdmittance(ppc):\n",
        "  ppc = ext2int(ppc)\n",
        "  BInputbaseMVA, BInputbus, BInputbranch = ppc['baseMVA'], ppc['bus'],  ppc['branch']\n",
        "\n",
        "  Bbus, Bf, Pbusinj, Pfinj = makeBdc(BInputbaseMVA, BInputbus, BInputbranch)\n",
        "  Bbus = csr_matrix(Bbus).toarray() #convert the csr_matrix into an array \n",
        "  #print(Bbus)\n",
        "  return Bbus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4QSjKmcz-Pd"
      },
      "source": [
        "Importing the Pd, Pg and Va csv files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaatyBHx0UHC"
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p_cy-LV1unG"
      },
      "source": [
        "def InitializeDataframes(): \n",
        "  PDdf = pd.read_csv('Pd.csv')\n",
        "  PGdf = pd.read_csv('Pg.csv')\n",
        "  VAdf = pd.read_csv('Va.csv')\n",
        "  #Successdf = pd.read_csv('Success.csv')\n",
        "  #print(PGdf)\n",
        "  #print(PDdf)\n",
        "  unNormalizedPD = PDdf\n",
        "  unNormalizedPG = PGdf\n",
        "  return PDdf, PGdf, VAdf, unNormalizedPD, unNormalizedPG "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-z87u_9ruOb"
      },
      "source": [
        "def GetMaxandMins(PDdf,slack_adapted):\n",
        "  #Getting PD_maxs and PD_mins for later use\n",
        "  PD_maxs = []\n",
        "  PD_mins = []\n",
        "  it =0\n",
        "  for column in PDdf.columns:\n",
        "          if(it!= slack_adapted):\n",
        "            PD_maxs.append(PDdf[column].max()) \n",
        "            PD_mins.append(PDdf[column].min())\n",
        "          it = it +1\n",
        "  #print(PD_maxs)\n",
        "  return PD_maxs, PD_mins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQyU33P22CGp"
      },
      "source": [
        "To Per Unit Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYNCJPRN0WSb"
      },
      "source": [
        "def To_Per_Unit(P, ppc):\n",
        "  P = P /  ppc['baseMVA']\n",
        "  return P"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NQ1bydQ1_rf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "def To_Per_Unit_Torch(P,ppc):\n",
        "  N= np.zeros((1,1))\n",
        "  N[0][0] =  ppc['baseMVA']\n",
        "  N = torch.from_numpy(N)\n",
        "  PerUnit = torch.divide(P,N)\n",
        "  return PerUnit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1Tx0zYD2Ku2"
      },
      "source": [
        "Load sampling and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWTrQIhs2SDy"
      },
      "source": [
        "def normalize(df):\n",
        "    # copy the dataframe\n",
        "    df_norm = df.copy()\n",
        "    # apply min-max scaling\n",
        "    for column in df_norm.columns:\n",
        "        if(df_norm[column].max()!=0):\n",
        "          df_norm[column] = (df_norm[column] - df_norm[column].min()) / (df_norm[column].max() - df_norm[column].min())\n",
        "        \n",
        "    return df_norm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rqw5yWo34QI"
      },
      "source": [
        "*Linear transformation and mapping dimension reduction*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPHOyOhyD2HU"
      },
      "source": [
        "def getslackindex(PGdf,slack):\n",
        "  PGdf2=PGdf.copy()\n",
        "  titles=list(PGdf2.columns)\n",
        "  strslack=str(int(slack))\n",
        "  temp=0\n",
        "  t=0\n",
        "  for i in range(len(titles)):\n",
        "    if(titles[i]==strslack):\n",
        "      temp1=titles[i]\n",
        "      t=i\n",
        "  return t\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXUOsvwy34YG"
      },
      "source": [
        "\n",
        "def GetAlphafromPG(PGdf,Power):\n",
        "  Alpha = PGdf.copy()\n",
        "  i = 0\n",
        "  for column in PGdf: \n",
        "    Alpha[column] = Alpha[column] - Power.at[i, \"Pmin\"]\n",
        "    Alpha[column] = Alpha[column]/ (Power.at[i, \"Pmax\"]- Power.at[i, \"Pmin\"])\n",
        "    Pmax=Power.at[i,\"Pmax\"]  \n",
        "    print(Pmax)\n",
        "    i= i+1    \n",
        "  return Alpha\n",
        "\n",
        "def GetAlphafromPGws(PGdf,Power,slack):\n",
        "  t=getslackindex(PGdf,slack)\n",
        "  print(PGdf)\n",
        "  Alpha = PGdf.copy()\n",
        "  Alpha = pd.DataFrame(Alpha)\n",
        "  slackstring = str(int(slack))\n",
        "  print(Alpha)\n",
        "  Alpha = Alpha.drop(labels= slackstring , axis=1,inplace=False)\n",
        "  i = 0\n",
        "  for column in PGdf: \n",
        "    if(i == t):\n",
        "      i = i+1\n",
        "    else:\n",
        "      Alpha[column] = Alpha[column] - Power.at[i, \"Pmin\"]\n",
        "      Alpha[column] = Alpha[column]/ (Power.at[i, \"Pmax\"]- Power.at[i, \"Pmin\"])\n",
        "      Pmax=Power.at[i,\"Pmax\"] \n",
        "      i= i+1  \n",
        "  return Alpha\n",
        "\n",
        "\n",
        "def GetPGfromAlpha(Alpha,Power, PGdf):\n",
        "  i = 0\n",
        "  PG = Alpha.copy()\n",
        "  for column in PG:\n",
        "    PG[column] = PG[column]* (Power.at[i, \"Pmax\"]- Power.at[i, \"Pmin\"])\n",
        "    PG[column] = PG[column] + Power.at[i, \"Pmin\"]\n",
        "    i= i+1\n",
        "  return PG\n",
        "\n",
        "def GetPGfromAlphaws(Alpha,Power,slack, PGdf):\n",
        "  t=getslackindex(PGdf,slack)\n",
        "  PG  = Alpha.copy()\n",
        "  Pdiff = []\n",
        "  Pmin = []\n",
        "  i = 0\n",
        "  for column in PG:\n",
        "    if (i==t):\n",
        "      i = i+1\n",
        "    else:\n",
        "      PG[column] = PG[column] * (Power.at[i, \"Pmax\"]- Power.at[i, \"Pmin\"])\n",
        "      PG[column] = PG[column] + Power.at[i, \"Pmin\"]\n",
        "      Pmax=Power.at[i,\"Pmax\"] \n",
        "      i= i+1\n",
        "  return PG\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COqytp1x5U_t"
      },
      "source": [
        "Removing the slack bus from  PD , PG:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP3TZI-2ALTy"
      },
      "source": [
        "def RemovingSlackFromPDPG(PDdf,PGdf,slack, unNormalizedPD):\n",
        "  slackstring = str(int(slack))\n",
        "  PDdf_ws=PDdf.drop(labels= slackstring , axis=1,inplace=False)\n",
        "  #print(PDdf_ws) #PD without slack\n",
        "  PGdf_ws=PGdf.drop(labels= slackstring , axis=1,inplace=False)\n",
        "  #print(PGdf_ws) #PG without slack\n",
        "  # PG NOT ALPHAC HERE CARFUL\n",
        "  unNormalizedPD_ws = unNormalizedPD.drop(labels= slackstring , axis=1,inplace=False)\n",
        "  #unNormalizedPG_ws = unNormalizedPG.drop(labels= slackstring , axis=1,inplace=False)\n",
        "  return PDdf_ws, PGdf_ws, unNormalizedPD_ws\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD1bGcgpAUv7"
      },
      "source": [
        "Removing the slack bus from admittance matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbhmkHK7BgTx"
      },
      "source": [
        "def RemovingSlack(slack_adapted, Bbus):\n",
        "  B_tilda = np.delete(Bbus, slack_adapted,0) \n",
        "  B_tilda = np.delete(B_tilda, slack_adapted,1)\n",
        "  return B_tilda "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCrICTa4Bt7i"
      },
      "source": [
        "Getting the Phase Angle (without the slack bus), for uploaded values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX-p-u2qIfL-"
      },
      "source": [
        "def Get_B_Matrix_Inverse(slack_adapted,ppc):\n",
        "  Bbus = GetAdmittance(ppc)\n",
        "  B_tilda = RemovingSlack(slack_adapted, Bbus)\n",
        "  #First need to invert the admittance matrix found previously\n",
        "  B_inverse = np.linalg.inv(B_tilda) \n",
        "  return B_inverse \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2qViqGMHWqG"
      },
      "source": [
        "def Get_Phase_Angles(PG, unNormalizedPD_ws, slack_adapted,ppc):\n",
        "  \n",
        "  B_inverse= Get_B_Matrix_Inverse(slack_adapted,ppc)\n",
        "  PDtemp = unNormalizedPD_ws.copy() # for this formula we need the unnormalized PD\n",
        "  #preparing the PG matrix\n",
        "  PG_full = pd.DataFrame(np.zeros(PDtemp.shape), columns = PDtemp.columns.tolist()) \n",
        "  PG_gen_list = PG.columns.tolist()\n",
        "  for col in PG_full.columns:\n",
        "    if col in PG_gen_list:\n",
        "      PG_full[col].values[:] = PG[col].values[:] \n",
        "\n",
        "  #Preparing PD and PG\n",
        "  PDtemp= To_Per_Unit(PDtemp,ppc) #turning PDs into per unit value\n",
        "  PG_full= To_Per_Unit(PG_full,ppc) #turning the P generated values\n",
        "  PDtemp = np.transpose(pd.DataFrame.to_numpy(PDtemp)) #transposing to have the buses as the rows and the cases as the columns\n",
        "  PG_full= np.transpose(pd.DataFrame.to_numpy(PG_full)) #transposing to have the buses as the rows and the cases as the columns\n",
        "  #getting the difference between PG and PD\n",
        "  Difference = PG_full - PDtemp\n",
        " \n",
        "  #getting the angles in degrees\n",
        "  Angles =(180/math.pi)*np.matmul(B_inverse,Difference)\n",
        "  \n",
        "  return Angles\n",
        "#Angles0 = Get_Phase_Angles(PGdf_ws, unNormalizedPD_ws)\n",
        "#print(Angles0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEFjSllKxIt"
      },
      "source": [
        "Getting the Phase Angle (with Slack)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DETuMiPHKwfR"
      },
      "source": [
        "def getAnglesWithSlack(Angles):\n",
        "  #We have to add 0s for the slack's phase angle \n",
        "  numberofrowsA , numberofcolumnsA= Angles.shape \n",
        "  numberofcolumnsA = numberofcolumnsA\n",
        "  slackangles = [[0]*numberofcolumnsA]\n",
        "  #print(slackangles)\n",
        "  #np.append(Angles,slackangles,0)\n",
        "  Angles = np.insert(Angles, int(slack_adapted) , slackangles , 0)\n",
        "  return Angles\n",
        "#Angles0 = getAnglesWithSlack(Angles0)\n",
        "#print(Angles0)\n",
        "#print(getAnglesWithSlack(Angles))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr68reeJO5Pd"
      },
      "source": [
        "Loss Functions:\n",
        "Getting LPG:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dNa10pLO9zM"
      },
      "source": [
        "#Penalty Function\n",
        "def penalty(x):\n",
        "  p = np.square(x) - 1\n",
        "  return p\n",
        "\n",
        "#A Matrix Function\n",
        "def createAmatrix(ppc):\n",
        "  n1, m1 = ppc['bus'].shape\n",
        "  n2, m2 = ppc['branch'].shape\n",
        "  A = np.empty((n2,n1))\n",
        "  A[:] = 0\n",
        "  Adf = pd.DataFrame(A)\n",
        "  for row in range(len(Adf)):\n",
        "    #print (ppc['branch'][row][0],row)\n",
        "    #print (ppc['branch'][row][1],row)\n",
        "    Adf[ppc['branch'][row][0]-1][row]= 1/ ((ppc['branch'][row][3]) * (To_Per_Unit(ppc['branch'][row][5],ppc))) #ai = 1/ xij Pijmax and ask prof jabr if the power in pu or angles in degrees or radiant \n",
        "    Adf[ppc['branch'][row][1]-1][row]= -1 / ((ppc['branch'][row][3]) * (To_Per_Unit(ppc['branch'][row][5],ppc))) #aj = - 1/ xij Pijmax \n",
        "    #print(Adf[ppc['branch'][row][0]-1][row])\n",
        "    #print(Adf[ppc['branch'][row][1]-1][row])\n",
        "  A = pd.DataFrame(Adf).to_numpy(dtype= 'float')\n",
        "  return A\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV5phN26yx8t"
      },
      "source": [
        "Preparing the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdVSfx26oALQ"
      },
      "source": [
        "def PreparingDnnData(PDdf_ws,PGdf,Power,slack):\n",
        "  #drop the  0 values and you are left with 20 colums --> input to 1st layer like in the paper \n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "\n",
        "  PDdf_withoutzeros= PDdf_ws.loc[:, (PDdf_ws != 0).any(axis=0)]\n",
        "  #PDdf_withoutzeros\n",
        "\n",
        "  Alpha = GetAlphafromPGws(PGdf, Power,slack)\n",
        "  #Alpha\n",
        "\n",
        "  inputsize = len(PDdf_withoutzeros.columns)\n",
        "  outputsize = len(Alpha.columns)\n",
        "  return PDdf_withoutzeros, Alpha, inputsize , outputsize \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKK529Av-bWe"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "def InitializeTrainTestData(test_size, PDdf_withoutzeros, Alpha):\n",
        "  PD_train, PD_test, Alpha_train, Alpha_test = train_test_split(PDdf_withoutzeros, Alpha, test_size= test_size, random_state=42)\n",
        "  return PD_train, PD_test, Alpha_train, Alpha_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcRYy55roGDr"
      },
      "source": [
        "Initialize the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRrfuNZq_4ZK"
      },
      "source": [
        "Training data to tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIaerknvokUi"
      },
      "source": [
        "def TrainingData(PD_train, Alpha_train):\n",
        "  #INPUTS \n",
        "  #Convert to Array \n",
        "  inputs = PD_train.values\n",
        "  inputs = inputs.astype ('float32') #emphasizing on the type\n",
        "\n",
        "  #Convert into tensors\n",
        "  inputs = torch.from_numpy(inputs)\n",
        "  #print(inputs)\n",
        "\n",
        "  #TARGETS\n",
        "  #Convert to Array \n",
        "  Alpha_target = Alpha_train.values\n",
        "  Alpha_target = Alpha_target.astype ('float32') #emphasizing on the type\n",
        "\n",
        "  #Convert into tensors\n",
        "  Alpha_target = torch.from_numpy(Alpha_target)\n",
        "  #print(Alpha_target)\n",
        "  return inputs, Alpha_target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aKNIp2C_8d4"
      },
      "source": [
        "Testing data to tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUgJvHObCyFG"
      },
      "source": [
        "def TestingData(PD_test, Alpha_test):\n",
        "  #INPUTS \n",
        "  #Convert to Array \n",
        "  inputs_test = PD_test.values\n",
        "  inputs_test = inputs_test.astype ('float32') #emphasizing on the type\n",
        "\n",
        "  #Convert into tensors\n",
        "  inputs_test = torch.from_numpy(inputs_test)\n",
        "  #print(inputs_test)\n",
        "\n",
        "  #TARGETS\n",
        "  #Convert to Array \n",
        "  Alpha_target_test = Alpha_test.values\n",
        "  Alpha_target_test = Alpha_target_test.astype ('float32') #emphasizing on the type\n",
        "\n",
        "  #Convert into tensors\n",
        "  Alpha_target_test = torch.from_numpy(Alpha_target_test)\n",
        "  #print(Alpha_target_test)\n",
        "  return inputs_test, Alpha_target_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZpINN_wEj4J"
      },
      "source": [
        "class DeepOPF(nn.Module):\n",
        "\n",
        "  def __init__(self,inputSize,outputSize,hiddenSize1,hiddenSize2, nbhidden):\n",
        "    super(DeepOPF, self).__init__()\n",
        "    self.function1 = nn.Linear(inputSize,hiddenSize1)\n",
        "    self.function2 = nn.Linear(hiddenSize1,hiddenSize2)\n",
        "    self.function3 = nn.Linear(hiddenSize2,outputSize)\n",
        "    self.nbhidden = nbhidden\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = torch.relu(self.function1(x))\n",
        "    for i in range(self.nbhidden):\n",
        "      x = torch.relu(self.function2(x))\n",
        "    x = torch.sigmoid(self.function3(x))\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIj-Ky8r1q9d"
      },
      "source": [
        "def Parameters_init(ppc,PD_maxs, PD_mins, Power, slack_adapted):\n",
        "  A= createAmatrix(ppc) #getting A matrix \n",
        "  Ones = np.zeros(shape=(np.shape(A)[0],1))\n",
        "  A = A.astype(np.float32)\n",
        "  A = torch.from_numpy(A) # A matrix as a tensor\n",
        "\n",
        "  #getting ones for loss 2\n",
        "  for x in range(len(Ones)):\n",
        "    Ones[x] =1\n",
        "  Ones = torch.from_numpy(Ones)\n",
        "\n",
        "  #getting the difference for unnormalization\n",
        "  PDdiff = []\n",
        "  PDdifftorch = []\n",
        "  it = 0\n",
        "  for column in PD_maxs:\n",
        "    PDdifftorch.append(PD_maxs[it]  - PD_mins[it] )\n",
        "    it = it +1\n",
        "  print(PDdifftorch)\n",
        "  PDdiff = PDdifftorch.copy()\n",
        "  PDdifftorch = torch.from_numpy(np.asarray(PDdifftorch, dtype = 'float64')) \n",
        "  print(PDdifftorch)\n",
        "\n",
        "  # getting min for unnormalization \n",
        "\n",
        "  PDmintorch = torch.from_numpy(np.asarray(PD_mins, dtype = 'float64'))  \n",
        "\n",
        "  #getting PG diff for getting PG from alphas\n",
        "\n",
        "  PGdifftorch = []\n",
        "  PGmin  = []\n",
        "  it=0\n",
        "  PGmax =  Power[:][\"Pmax\"]\n",
        "  for column in PGmax:\n",
        "      if (it != slack_adapted):\n",
        "        PGdifftorch.append(Power.at[it, \"Pmax\"]- Power.at[it, \"Pmin\"])\n",
        "        PGmin.append(Power.at[it,\"Pmin\"])\n",
        "      it= it+1\n",
        "  PGdifftorch = torch.from_numpy(np.asarray(PGdifftorch, dtype = 'float64'))\n",
        "\n",
        "  #getting PG min for getting PG from alphas\n",
        "\n",
        "  PGmintorch = torch.from_numpy(np.asarray(PGmin, dtype = 'float64'))\n",
        "\n",
        "  #getting the B inverse for later use\n",
        "\n",
        "  B_inverse = Get_B_Matrix_Inverse(slack_adapted,ppc)\n",
        "  B_inverse = torch.from_numpy(B_inverse)\n",
        "  return B_inverse, PGmintorch, PGdifftorch, PDmintorch, PDdifftorch, Ones, A , PDdiff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpkrGGHhdMk5"
      },
      "source": [
        "def Train(slack_adapted, n_epochs, DataLoad,optimizer, Deep_OPF, B_inverse, PGmintorch, PGdifftorch, PDmintorch, PDdifftorch, Ones, A, PDdiff, unNormalizedPD_ws, PGdf_ws, ppc):\n",
        "  from time import time\n",
        "  t = time()\n",
        "  for epochs in range(n_epochs):\n",
        "    iteration = 0\n",
        "    Loss_per_batch = []\n",
        "    for i,(inputs,Alpha_target) in enumerate(DataLoad):\n",
        "      optimizer.zero_grad()\n",
        "      output_pred = Deep_OPF(inputs)\n",
        "      loss1 = torch.nn.MSELoss(size_average=None, reduce=None, reduction = 'mean') #\n",
        "      Loss1_input = loss1(output_pred ,Alpha_target) \n",
        "      #getting PG from output alpha\n",
        "      PGOut0 = torch.mul(output_pred, PGdifftorch)\n",
        "      PGOut0 = torch.add(PGOut0, PGmintorch)\n",
        "      #getting PDs from input tensors \n",
        "      PDIn = torch.zeros(1,len(unNormalizedPD_ws.columns.to_list()))\n",
        "      PGout = torch.zeros(1,len(unNormalizedPD_ws.columns.to_list()))\n",
        "      columnOfInput = 0\n",
        "      PDbuscolumn =0\n",
        "      #adding the 0s\n",
        "      for i in unNormalizedPD_ws:\n",
        "        if (unNormalizedPD_ws[i].max() != 0):\n",
        "          PDIn[0,PDbuscolumn]= inputs[0,columnOfInput]\n",
        "          columnOfInput = columnOfInput + 1\n",
        "        PDbuscolumn =   PDbuscolumn + 1\n",
        "          #unNormalizing PD\n",
        "      PDIn = torch.mul(PDIn, PDdifftorch)\n",
        "      PDIn = torch.add(PDIn, PDmintorch)\n",
        "      columnOfOutput = 0\n",
        "      PGbuscolumn =0\n",
        "      #getting theta tilda\n",
        "      PG_gen_list = PGdf_ws.columns.tolist()\n",
        "      for col in unNormalizedPD_ws.columns:\n",
        "        if col in PG_gen_list:\n",
        "          PGout[0,PGbuscolumn] = PGOut0[0,columnOfOutput]\n",
        "          columnOfOutput = columnOfOutput + 1\n",
        "        PGbuscolumn =   PGbuscolumn + 1\n",
        "      # preparing PD and PG in per unit\n",
        "      PDInFinal =  To_Per_Unit_Torch(PDIn,ppc)\n",
        "      PGOutFinal = To_Per_Unit_Torch(PGout,ppc)\n",
        "      PDInFinal = torch.transpose(PDInFinal, 0, 1)\n",
        "      PGOutFinal = torch.transpose(PGOutFinal, 0, 1)\n",
        "      Difference = torch.subtract(PGOutFinal,PDInFinal) #getting the difference between PG and PD\n",
        "      Angles =torch.matmul(B_inverse,Difference)   #getting the angles in randiants / multiply by 180/pi for randiants\n",
        "      length_withslack = 1+len(unNormalizedPD_ws.columns.to_list())\n",
        "      Angles_hat = torch.zeros([length_withslack,1] )\n",
        "      #getting angles with slack\n",
        "      it=0\n",
        "      for i in range(length_withslack):\n",
        "        if i != slack_adapted:\n",
        "        \n",
        "          Angles_hat[i,0] = Angles[it,0]\n",
        "          it = it +1\n",
        "      \n",
        "      #calculating loss 2\n",
        "      Loss2_input = torch.matmul(A,Angles_hat)    \n",
        "      Loss2_input = torch.square(Loss2_input)\n",
        "      Loss2_input = torch.subtract(Loss2_input,Ones)\n",
        "      Loss2_input = torch.mean(Loss2_input)\n",
        "      Total_Loss = Loss1_input + Loss2_input* 0.00001\n",
        "      Total_Loss.backward()#\n",
        "      optimizer.step()#\n",
        "    \n",
        "    print(epochs, Loss1_input.item())\n",
        "    \n",
        "\n",
        "  print(f'Time to run: {(time() - t)} seconds')\n",
        "  return\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybVphAzAtmMd"
      },
      "source": [
        "def Test(inputs_test, Alpha_target_test, PD_test, Deep_OPF):\n",
        "  from time import time\n",
        "  from numpy import vstack\n",
        "  import numpy as np\n",
        "  import math\n",
        "\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  test_data = []\n",
        "  for i in range(len(PD_test)):\n",
        "      test_data.append([inputs_test[i],Alpha_target_test[i]])\n",
        "  TestLoad = torch.utils.data.DataLoader(dataset = test_data, batch_size = 64, shuffle = True)\n",
        "  predictions, actuals = list(), list()\n",
        "  t = time()\n",
        "  for i, (inputs_test, Alpha_target_test) in enumerate(test_data):\n",
        "      # evaluate the model on the test set\n",
        "      yhat = Deep_OPF(inputs_test)\n",
        "      # retrieve numpy array\n",
        "      yhat = yhat.detach().numpy()\n",
        "      actual = Alpha_target_test.numpy()\n",
        "      predictions.append(yhat)\n",
        "      actuals.append(actual)\n",
        "  print(f'Time to run: {(time() - t)} seconds')\n",
        "  return actuals, predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FjentWlBMr7"
      },
      "source": [
        "Average of Root Mean Squared error of 15000 testing simulations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amMMq--0G-rr"
      },
      "source": [
        "def AlphaRMSE(actuals, predictions):\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  n=[]\n",
        "  actuals = np.array(actuals)\n",
        "  predictions = np.array(predictions)\n",
        "  for i in range(len(actuals[0])):\n",
        "    s=mean_squared_error(actuals[i],predictions[i],squared=False)\n",
        "    n.append(s)\n",
        "  n=np.array(n)\n",
        "  avg = n.sum()/len(n)\n",
        "  return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-Y6_X00Fiz9"
      },
      "source": [
        "Average R2 SCORE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdELLFSKFzg8"
      },
      "source": [
        "def AlphaR2Score(actuals,predictions):\n",
        "  from sklearn.metrics import r2_score\n",
        "  n=[]\n",
        "  actuals = np.array(actuals)\n",
        "  predictions = np.array(predictions)\n",
        "  for i in range(len(actuals[0])):\n",
        "    c=r2_score(actuals[i], predictions[i])\n",
        "    n.append(c)\n",
        "  n=np.array(n) \n",
        "  avg = n.sum()/len(n) \n",
        "  return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIoucdnbzet-"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKuGcC0py8S8"
      },
      "source": [
        "def SaveModel(Deep_OPF):\n",
        "  PATH= \"model.pth\"\n",
        "  torch.save(Deep_OPF.state_dict(), PATH)\n",
        "  return PATH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsTQov6YZkJY"
      },
      "source": [
        "Gettig the real Pg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFUME7QT5HRg"
      },
      "source": [
        "def GetPGsWithoutSlack(actuals,slack, Power, PGdf,predictions):\n",
        "  actuals_df = pd.DataFrame(actuals)\n",
        "  PG_actual_ws = GetPGfromAlphaws(actuals_df,Power,slack, PGdf)\n",
        "  print(\"PG_actual_ws\")\n",
        "  print(PG_actual_ws)\n",
        "  predictions_df = pd.DataFrame(predictions)\n",
        "  PG_predicted_ws = GetPGfromAlphaws(predictions_df,Power,slack, PGdf)\n",
        "  print(\"PG_predicted_ws\")\n",
        "  print(PG_predicted_ws)\n",
        "  #PG_predicted_ws.to_csv(r'PG_predictions_ws.csv',index=False)\n",
        "  return PG_predicted_ws,  PG_actual_ws"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aLL2sdD9sNB"
      },
      "source": [
        "Getting the phase angles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA1O8THh9v7r"
      },
      "source": [
        "def GetAnglesWithoutSlack(PG_actual_ws, PD_test,PDdiff, unNormalizedPD_ws,PD_mins, PG_predicted_ws,slack_adapted,ppc):\n",
        "  R = len(PD_test.index)\n",
        "  C= len(unNormalizedPD_ws.columns)\n",
        "  PD_test_unNormalized = np.zeros((R,C))\n",
        "  PD_test = PD_test.to_numpy()\n",
        "  columnOfInput = 0\n",
        "  PDbuscolumn = 0\n",
        "  for i in unNormalizedPD_ws:\n",
        "        if (unNormalizedPD_ws[i].max() != 0):\n",
        "          for row in range(R):\n",
        "            PD_test_unNormalized[row,PDbuscolumn]= PD_test[row,columnOfInput]\n",
        "            PD_test_unNormalized[row,PDbuscolumn] = PD_test_unNormalized[row,PDbuscolumn] * PDdiff[PDbuscolumn]\n",
        "            PD_test_unNormalized[row,PDbuscolumn] = PD_test_unNormalized[row,PDbuscolumn] + PD_mins[PDbuscolumn]\n",
        "          columnOfInput = columnOfInput + 1\n",
        "        PDbuscolumn =   PDbuscolumn + 1\n",
        "\n",
        "  PD_test_unNormalized =  pd.DataFrame(PD_test_unNormalized)\n",
        "  #print(PD_test)\n",
        "  #print(PD_test_unNormalized)\n",
        "  #print(PG_actual_ws)\n",
        "  angles_actual=Get_Phase_Angles(PG_actual_ws, PD_test_unNormalized,slack_adapted,ppc)\n",
        "  angles_actual = pd.DataFrame(angles_actual)\n",
        "  angles_actual = angles_actual.T \n",
        "  #print(angles_actual)\n",
        "  #angles_actual.to_csv(r'Angles_actuals_ws.csv',index=False)\n",
        "\n",
        "  angles_predicted=Get_Phase_Angles(PG_predicted_ws, PD_test_unNormalized,slack_adapted,ppc)\n",
        "  angles_predicted = pd.DataFrame(angles_predicted)\n",
        "  angles_predicted = angles_predicted.T \n",
        "  #print(angles_predicted)\n",
        "  #angles_predicted.to_csv(r'Angles_predictions_ws.csv',index=False)\n",
        "  return angles_actual , angles_predicted, PD_test_unNormalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5aGnQ6R1wvn"
      },
      "source": [
        "Average of Root Mean Squared error of 15000 Angles and PG Testing Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6nz4RL1v63"
      },
      "source": [
        "def RMSEPGAngles(PG_actual_ws,PG_predicted_ws,angles_actual,angles_predicted):\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  n1=[]\n",
        "  n2=[]\n",
        "  actualAngles = np.array(angles_actual)\n",
        "  predictionAngles = np.array(angles_predicted)\n",
        "  actualPG = np.array(PG_actual_ws)\n",
        "  predictionPG = np.array(PG_predicted_ws)\n",
        "  print(len(actualPG[0]))\n",
        "  for i in range(len(actualPG[0])):\n",
        "    s1=mean_squared_error(actualPG[i],predictionPG[i],squared=False)\n",
        "    n1.append(s1)\n",
        "  n1=np.array(n1)\n",
        "  print(\"RMSE For PG:\")\n",
        "  #print(n1)\n",
        "  print(n1.sum()/len(n1))\n",
        "  avgP = n1.sum()/len(n1)\n",
        "  for i in range(len(actualAngles[0])):\n",
        "    s2=mean_squared_error(actualAngles[i],predictionAngles[i],squared=False)\n",
        "    n2.append(s2)\n",
        "  n2=np.array(n2)\n",
        "  print(\"RMSE For Angles:\")\n",
        "  #print(n2)\n",
        "  print(n2.sum()/len(n2))\n",
        "  avgA = n2.sum()/len(n2)\n",
        "  return avgA, avgP, actualPG, actualAngles, predictionPG, predictionAngles\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nQNge8s8OXe"
      },
      "source": [
        "Average R2 SCORE "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHtlgF3X8PM9"
      },
      "source": [
        "def R2PGAngles(actualPG,predictionPG,actualAngles,predictionAngles):\n",
        "  from sklearn.metrics import r2_score\n",
        "  n1= []\n",
        "  n2 = []\n",
        "  for i in range(len(actualPG[0])):\n",
        "    c1=r2_score(actualPG[i],predictionPG[i])\n",
        "    n1.append(c1)\n",
        "  n1=np.array(n1)\n",
        "\n",
        "  print(\"R2 For PG:\")\n",
        "  print(n1)\n",
        "  print(n1.sum()/len(n1))\n",
        "  for i in range(len(actualAngles[0])):\n",
        "    c2=r2_score(actualAngles[i],predictionAngles[i])\n",
        "    n2.append(c2)\n",
        "  print(\"R2 For Angles:\")\n",
        "\n",
        "  n2=np.array(n2) \n",
        "  #print(n2)\n",
        "  print((n2.sum()/len(n2)))\n",
        "  avgP = n1.sum()/len(n1)\n",
        "  avgA = n2.sum()/len(n2)\n",
        "  return avgA, avgP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY-MoMxrBxkE"
      },
      "source": [
        "Getting PGi of the slack bus is obtained by subtracting output of the other\n",
        "buses from the total load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA-4rwwnCAj7"
      },
      "source": [
        " def GettingSlackPG(PG_predicted_ws, PD_test_unNormalized):\n",
        "  import pandas as pd\n",
        "  PG_predicted_ws = pd.DataFrame(PG_predicted_ws.copy())\n",
        "  PD_test_unNormalized=pd.DataFrame(PD_test_unNormalized.copy())\n",
        "  PD_test_unNormalized[\"Total D\"] = PD_test_unNormalized.sum(axis=1)\n",
        "  PG_predicted_ws[\"Total G\"] = PG_predicted_ws.sum(axis=1)\n",
        "  print(PD_test_unNormalized)\n",
        "  print(PG_predicted_ws)\n",
        "  #for column in PD_test_unNormalized[['Total D']]:\n",
        "  #for column in PG_predicted_ws[['Total G']]:\n",
        "  slack_generation=PD_test_unNormalized[\"Total D\"] - PG_predicted_ws[\"Total G\"]\n",
        "  print(slack_generation)\n",
        "  return slack_generation, PG_predicted_ws"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2XqNrzY6PpW"
      },
      "source": [
        "insert in Pg without slack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXvneyzumxn2"
      },
      "source": [
        "def AddingSlackToPG(PG_predicted_ws,slack_generation, slack_adapted,slack,ppc):\n",
        "  #print(PG_predicted_ws)\n",
        "  PG_predicted_ws.insert(slack_adapted,str(slack),slack_generation)\n",
        "  #print(PG_predicted_ws)\n",
        "  PG_predicted_ws = PG_predicted_ws.drop([\"Total G\"], axis=1)\n",
        "  #print(PG_predicted_ws)\n",
        "  columnnames= []\n",
        "  for row in ppc[\"gen\"]:\n",
        "    columnnames.append(row[0])\n",
        "  PredictedPG_WithSlack = pd.DataFrame(PG_predicted_ws.to_numpy(), columns=columnnames)\n",
        "  #print(Predicted_WithSlack)\n",
        "  return PredictedPG_WithSlack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mky6rWBoktBz"
      },
      "source": [
        "def AddingSlacktoAngles(predictionAngles, slack_adapted, slack,ppc):\n",
        "  predictionAngles = pd.DataFrame(predictionAngles)\n",
        "  slackdf = pd.DataFrame(0, index=range(len(predictionAngles)), columns=[str(slack)])\n",
        "  predictionAngles.insert(slack_adapted,str(slack),slackdf)\n",
        "  columnnames= []\n",
        "  for row in ppc[\"bus\"]:\n",
        "    columnnames.append(row[0])\n",
        "  PredictedAngles_WithSlack = pd.DataFrame(predictionAngles.to_numpy(), columns=columnnames)\n",
        "  return PredictedAngles_WithSlack "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Q92s3uc2pX"
      },
      "source": [
        "Cost Function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9s0QbGOc4Vu"
      },
      "source": [
        "def GenerationCost(PG,ppc):\n",
        "  PGCost = PG.copy()\n",
        "  PGCost = PGCost.to_numpy()\n",
        "  for col in range(PGCost.shape[1]):\n",
        "    PGCost[:,col] = (PGCost[:,col]) * (PGCost[:,col]) * ppc[\"gencost\"][col][4] + PGCost[:,col] *  ppc[\"gencost\"][col][5] + ppc[\"gencost\"][col][6]\n",
        "  PGCost= pd.DataFrame(PGCost)\n",
        "  PGCost[\"Total Cost\"] = PGCost.sum(axis=1)\n",
        "  PGCostAvg = PGCost[\"Total Cost\"].mean()\n",
        "  return PGCost, PGCostAvg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRbAfJBfliOn"
      },
      "source": [
        "def TestingAndResults(PD_test, actuals, predictions , slack_adapted, slack, Power, ppc, Deep_OPF, PGdf, PDdiff, unNormalizedPD_ws, PD_mins):\n",
        "  #AlphaRMSE = AlphaRMSE(actuals, predictions)\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  n1=[]\n",
        "  actualstest = np.array(actuals.copy())\n",
        "  predictionstest = np.array(predictions.copy())\n",
        "  for i in range(len(actualstest[0])):\n",
        "    s1=mean_squared_error(actualstest[i],predictionstest[i],squared=False)\n",
        "    n1.append(s1)\n",
        "  n1=np.array(n1)\n",
        "  AlphaRMSE = n1.sum()/len(n1)\n",
        "\n",
        "\n",
        "  AlphaR2 = AlphaR2Score(actuals,predictions)\n",
        "  Path = SaveModel(Deep_OPF)\n",
        "  PG_predicted_ws,  PG_actual_ws = GetPGsWithoutSlack(actuals, slack , Power, PGdf,predictions)\n",
        "  angles_actual , angles_predicted, PD_test_unNormalized = GetAnglesWithoutSlack(PG_actual_ws,PD_test,PDdiff, unNormalizedPD_ws, PD_mins, PG_predicted_ws, slack_adapted,ppc)\n",
        "  RMSETestingavgA, RMSETestingavgP, actualPG, actualAngles, predictionPG, predictionAngles = RMSEPGAngles(PG_actual_ws,PG_predicted_ws,angles_actual,angles_predicted)\n",
        "  R2avgA, R2avgP = R2PGAngles(actualPG,predictionPG,actualAngles,predictionAngles)\n",
        "  slack_generation, PG_predicted_ws = GettingSlackPG(PG_predicted_ws, PD_test_unNormalized)\n",
        "  PredictedPG_WithSlack =  AddingSlackToPG(PG_predicted_ws,slack_generation, slack_adapted,slack,ppc)\n",
        "  PredictedAngles_WithSlack = AddingSlacktoAngles(predictionAngles, slack_adapted, slack, ppc)\n",
        "  slack_generation_real,PG_actual_ws  = GettingSlackPG(PG_actual_ws, PD_test_unNormalized)\n",
        "  ActualPG_WithSlack =  AddingSlackToPG(PG_actual_ws,slack_generation_real, slack_adapted,slack,ppc)\n",
        "  ActualAngles_WithSlack = AddingSlacktoAngles(actualAngles, slack_adapted, slack, ppc)\n",
        "  PredictedPG_WithSlack.to_csv(r'PGPredictions.csv',index=False)\n",
        "  PredictedAngles_WithSlack.to_csv(r'AnglesPredictions.csv',index=False)\n",
        "  ActualPG_WithSlack.to_csv(r'ActualPGs.csv',index=False)\n",
        "  ActualAngles_WithSlack.to_csv(r'ActualAngles.csv',index=False)\n",
        "  return  PredictedPG_WithSlack,PredictedAngles_WithSlack, ActualAngles_WithSlack, ActualPG_WithSlack, Path, AlphaR2, AlphaRMSE, RMSETestingavgA, RMSETestingavgP,R2avgA, R2avgP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMcvhabFkS6i"
      },
      "source": [
        "def dnn(split_dataset, batch_size, epoch,learning_rate, hidden_layer,neurons):\n",
        "  #initialize ppc\n",
        "  from case import case\n",
        "  ppc = case()\n",
        "  test_size = split_dataset\n",
        "  hiddenSize1 = neurons\n",
        "  hiddenSize2 = neurons\n",
        "  batch_size = batch_size\n",
        "  learning_rate =learning_rate\n",
        "  n_epochs = epoch\n",
        "  nbhidden = hidden_layer\n",
        "  #test_size = 0.5\n",
        "  #inputSize = 20\n",
        "  #outputSize = 5\n",
        "  #hiddenSize1 = 16\n",
        "  #hiddenSize2 = 16\n",
        "  #batch_size = 64\n",
        "  #learning_rate = 1e-3\n",
        "  #n_epochs = 235\n",
        "  print(\"Initialization\")\n",
        "  Power,slack, slack_adapted = Initialize(ppc)\n",
        "  Bbus  = GetAdmittance(ppc)\n",
        "  PDdf, PGdf, VAdf, unNormalizedPD, unNormalizedPG = InitializeDataframes() #don't forget to set the csvs before\n",
        "  PD_maxs, PD_mins = GetMaxandMins(PDdf, slack_adapted)\n",
        "  PDdf = normalize(PDdf)\n",
        "  PDdf_ws, PGdf_ws, unNormalizedPD_ws = RemovingSlackFromPDPG(PDdf,PGdf,slack, unNormalizedPD)\n",
        "  B_tilda = RemovingSlack(slack_adapted, Bbus) \n",
        "  PDdf_withoutzeros, Alpha, inputSize , outputSize = PreparingDnnData(PDdf_ws,PGdf,Power,slack)\n",
        "  PD_train, PD_test, Alpha_train, Alpha_test = InitializeTrainTestData(test_size, PDdf_withoutzeros, Alpha) \n",
        "  inputs, Alpha_target = TrainingData(PD_train, Alpha_train) \n",
        "  inputs_test, Alpha_target_test = TestingData(PD_test, Alpha_test)\n",
        "  Deep_OPF = DeepOPF(inputSize,outputSize,hiddenSize1,hiddenSize2,nbhidden)\n",
        "  emp = []\n",
        "  for i in range(len(PD_train)):\n",
        "    emp.append([inputs[i],Alpha_target[i]])\n",
        "  DataLoad = torch.utils.data.DataLoader(dataset = emp, batch_size = batch_size, shuffle = True)\n",
        "  optimizer = torch.optim.SGD(Deep_OPF.parameters(), lr= learning_rate)\n",
        "  B_inverse, PGmintorch, PGdifftorch, PDmintorch, PDdifftorch, Ones, A, PDdiff = Parameters_init(ppc,PD_maxs, PD_mins, Power, slack_adapted)\n",
        "  Train(slack_adapted, n_epochs, DataLoad,optimizer, Deep_OPF, B_inverse, PGmintorch, PGdifftorch, PDmintorch, PDdifftorch, Ones, A, PDdiff, unNormalizedPD_ws, PGdf_ws, ppc)\n",
        "  actuals, predictions = Test(inputs_test, Alpha_target_test, PD_test, Deep_OPF)\n",
        "  print('done')\n",
        "  PredictedPG_WithSlack,PredictedAngles_WithSlack, ActualAngles_WithSlack, ActualPG_WithSlack, Path, AlphaR2Test, AlphaRMSETest, RMSETestingavgA, RMSETestingavgP,R2avgA, R2avgP = TestingAndResults(PD_test, actuals, predictions , slack_adapted, slack, Power, ppc, Deep_OPF, PGdf, PDdiff, unNormalizedPD_ws, PD_mins)\n",
        "  PGCost, PGCostAvg = GenerationCost(PredictedPG_WithSlack,ppc)\n",
        "  PGCostAct , PGCostAvgActual = GenerationCost(ActualPG_WithSlack, ppc)\n",
        "  PGCost.to_csv(r'PGCost.csv',index=False)\n",
        "  #return actuals, predictions , slack_adapted, slack, ppc, Deep_OPF , Power , PGdf, PD_test, PDdiff, unNormalizedPD_ws, PD_mins\n",
        "  return PGCostAvg ,  AlphaR2Test, AlphaRMSETest ,  PGCostAvgActual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJLjSPekOunD"
      },
      "source": [
        " PGCostAvg ,  AlphaR2, AlphaRMSE   ,  PGCostAvgActual= dnn(split_dataset = 0.5 , batch_size = 64,epoch = 230,learning_rate= 0.001, hidden_layer = 1 ,neurons = 16)\n",
        " print( PGCostAvg)\n",
        " print( PGCostAvgActual)\n",
        " print(AlphaR2)\n",
        " print( AlphaRMSE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGmITTnkfrbI"
      },
      "source": [
        "FLASK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c82czg4Urre-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76722a8b-c2ef-4289-c369-b5f0e5b74faf"
      },
      "source": [
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOslg3JOOupk"
      },
      "source": [
        "!pip install werkzeug "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqyNhWmsfw3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "0fb2e5d7-f88d-4b0d-dc92-475af7b7e0f2"
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "import threading\n",
        "from flask import Flask , render_template, request, send_file\n",
        "app = Flask(__name__, template_folder='/content/Templates')\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/training.html',methods = ['GET'])\n",
        "def train():\n",
        "    return render_template('training.html')\n",
        "\n",
        "@app.route('/Team.html',methods = ['GET'])\n",
        "def team():\n",
        "    return render_template('Team.html')\n",
        "\n",
        "@app.route('/index.html')\n",
        "def backhome():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/terminal',methods = ['POST','GET'])\n",
        "def terminal():\n",
        "    if request.method == 'POST':\n",
        "        #f1 = request.files['PD']\n",
        "        #f1.save(f1.filename)\n",
        "        #f2 = request.files['PG']\n",
        "        #f2.save(f2.filename)\n",
        "        #f3 = request.files['Va']\n",
        "        #f3.save(f3.filename)\n",
        "        #f4 = request.files['case']\n",
        "        #f4.save(f4.filename)\n",
        "        result = request.form\n",
        "        '''Read the values from HTML file and set the values for training.''' \n",
        "        result = request.form\n",
        "        split_dataset = result['split_dataset']\n",
        "        project_name = result['project_name']\n",
        "        batch_size = result['batch_size']\n",
        "        epoch = result['epoch']\n",
        "        learning_rate = result['learning_rate']\n",
        "        hidden_layer = result['hidden_layer'] \n",
        "        neurons = result['neurons']\n",
        "        #print(neurons)\n",
        "        PGCostAvg ,  AlphaR2, AlphaRMSE = dnn(split_dataset = float(split_dataset), batch_size = int(batch_size),epoch = int(epoch),learning_rate= float(learning_rate), hidden_layer = int(hidden_layer),neurons = int(neurons))\n",
        "        #threading.Thread(target=app.run).start()\n",
        "        R2_test = AlphaR2 *100\n",
        "        RMSE_test =  AlphaRMSE*100\n",
        "        cost= PGCostAvg *100\n",
        "        return render_template('terminal.html', result = result, RMSE_test=RMSE_test, R2_test = R2_test, cost = cost)\n",
        "\n",
        "@app.route('/savedmodel')\n",
        "def savedmodel():\n",
        "\n",
        "  return render_template('result.html')\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/download')\n",
        "def download_file():\n",
        "    path = \"/content/Templates/index.html\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_file_actualP():\n",
        "    path = \"/content/ActualPGs.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_file_actualA():\n",
        "    path = \"/content/ActualAngles.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_file_predictedA():\n",
        "    path = \"/content/AnglesPredictions.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_file_predictedP():\n",
        "    path = \"/content/PGPredictions.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_filePD():\n",
        "    path = \"/content/IndexTemp/PDTemp.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_filePG():\n",
        "    path = \"/content/IndexTemp/PGTemp.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_fileVa():\n",
        "    path = \"/content/IndexTemp/VaTemp.csv\"\n",
        "    return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_fileCase():\n",
        "   path = \"/content/IndexTemp/case.py\"\n",
        "   return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_filePDPredict():\n",
        "   path = \"/content/IndexTemp/PDpredict.csv\"\n",
        "   return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_file_model():\n",
        "   path = \"/content/model.pth\"\n",
        "   return send_file(path, as_attachment=True)\n",
        "@app.route('/download')\n",
        "def download_file_cost():\n",
        "  path = \"/content/PGCost.csv\"\n",
        "  return send_file(path, as_attachment=True)\n",
        "\n",
        "@app.route('/pre-trained',methods = ['POST','GET'])\n",
        "def pretrained():\n",
        "  if request.method == 'POST':\n",
        "     f1 = request.files['PD']\n",
        "     f1.save(f1.filename)\n",
        "     case = request.form['Pre-trained Model']\n",
        "     result30 = { 'project_name' : 'IEEE Case 30','split_dataset' : 0.5 , 'batch_size' : 64, 'epoch' : 235 , 'learning_rate' : 0.001,'hidden_layer': 2, 'neurons': 16  }\n",
        "     \n",
        "     result57 = { 'project_name' : 'IEEE Case 57','split_dataset' : 0.5 , 'batch_size' : 64, 'epoch' : 235 , 'learning_rate' : 0.001,'hidden_layer': 4, 'neurons': 32  }\n",
        "     \n",
        "     result118 = { 'project_name' : 'IEEE Case 30','split_dataset' : 0.5 , 'batch_size' : 64, 'epoch' : 235 , 'learning_rate' : 0.001,'hidden_layer': 6, 'neurons': 64  }\n",
        "     \n",
        "     result300 = { 'project_name' : 'IEEE Case 30','split_dataset' : 0.5 , 'batch_size' : 64, 'epoch' : 235 , 'learning_rate' : 0.001,'hidden_layer': 6, 'neurons': 128  }\n",
        "     \n",
        "     if (str(case) == 'IEEE Case 30'):\n",
        "       print('30')\n",
        "       #PGCostAvg ,  AlphaR2, AlphaRMSE = dnnforpretrained(30)\n",
        "       #R2_test = AlphaR2 *100\n",
        "       #RMSE_test =  AlphaRMSE*100\n",
        "       #cost= PGCostAvg *100\n",
        "       return render_template('result.html', result = result30) # , RMSE_test=RMSE_test, R2_test = R2_test, cost = cost)\n",
        "     ` \n",
        "     elif (str(case) == 'IEEE Case 57'):\n",
        "        print('57')\n",
        "        PGCostAvg ,  AlphaR2, AlphaRMSE = dnnforpretrained(57)\n",
        "        R2_test = AlphaR2 *100\n",
        "        RMSE_test =  AlphaRMSE*100\n",
        "        cost= PGCostAvg *100\n",
        "        return render_template('result.html', result = result57 , RMSE_test=RMSE_test, R2_test = R2_test, cost = cost)\n",
        "      \n",
        "      elif (str(case)== 'IEEE Case 118'):\n",
        "        print('118')\n",
        "        PGCostAvg ,  AlphaR2, AlphaRMSE = dnnforpretrained(118)\n",
        "        R2_test = AlphaR2 *100\n",
        "        RMSE_test =  AlphaRMSE*100\n",
        "        cost= PGCostAvg *100\n",
        "        return render_template('result.html', result = result118 , RMSE_test=RMSE_test, R2_test = R2_test, cost = cost)\n",
        "\n",
        "      if (str(case) == 'IEEE Case 300'):\n",
        "       PGCostAvg ,  AlphaR2, AlphaRMSE = dnnforpretrained(118)\n",
        "       R2_test = AlphaR2 *100\n",
        "       RMSE_test =  AlphaRMSE*100\n",
        "       cost= PGCostAvg *100\n",
        "       return render_template('result.html', result = result300 , RMSE_test=RMSE_test, R2_test = R2_test, cost = cost)\n",
        "       \n",
        "\n",
        "app.run()\n",
        "   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-70-e822769de817>\"\u001b[0;36m, line \u001b[0;32m131\u001b[0m\n\u001b[0;31m    elif (str(case) == 'IEEE Case 57'):\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_g3G9DPAlya"
      },
      "source": [
        "Checking for first PGi constraint violations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH1AwOHuAo96"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peUO3vqtBXTI"
      },
      "source": [
        "Checking for second Thetha constraint violation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGUYF_4UCBai"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaeO_c3mpwjt"
      },
      "source": [
        "slack_generation, PG_predicted_ws = GettingSlackPG(PG_predicted_ws, PD_test_unNormalized)\n",
        "PredictedPG_WithSlack =  AddingSlackToPG(PG_predicted_ws,slack_generation, slack_adapted,ppc)\n",
        "PredictedAngles_WithSlack = AddingSlacktoAngles(predictionAngles, slack_adapted, slack, ppc)\n",
        "slack_generation_real,PG_actual_ws  = GettingSlackPG(PG_actual_ws, PD_test_unNormalized)\n",
        "ActualPG_WithSlack =  AddingSlackToPG(PG_actual_ws,slack_generation_real, slack_adapted,ppc)\n",
        "ActualAngles_WithSlack = AddingSlacktoAngles(actualAngles, slack_adapted, slack, ppc)\n",
        "PredictedPG_WithSlack.to_csv(r'PGPredictions.csv',index=False)\n",
        "PredictedAngles_WithSlack.to_csv(r'AnglesPredictions.csv',index=False)\n",
        "ActualPG_WithSlack.to_csv(r'ActualPGs.csv',index=False)\n",
        "ActualAngles_WithSlack.to_csv(r'ActualAngles.csv',index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}